{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "num_of_json = 70\n",
    "file = 'yelp_dataset/yelp_academic_dataset_review.json'\n",
    "parsed_file = 'yelp_review_small.json'\n",
    "list_of_reviews_rate = []\n",
    "with open(file) as f:\n",
    "    with open(parsed_file, 'w') as outf:\n",
    "        for i, line in enumerate(f):\n",
    "            pl = json.loads(line)\n",
    "            json.dump({\"text\": pl[\"text\"], \"label\":pl[\"stars\"]}, outf)\n",
    "            outf.write('\\n')\n",
    "            if i==num_of_json:\n",
    "                break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ilannaiman/opt/anaconda3/envs/torchDP/lib/python3.7/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "/Users/ilannaiman/opt/anaconda3/envs/torchDP/lib/python3.7/site-packages/torchtext/data/field.py:150: UserWarning: LabelField class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "/Users/ilannaiman/opt/anaconda3/envs/torchDP/lib/python3.7/site-packages/torchtext/data/example.py:13: UserWarning: Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchtext import data\n",
    "\n",
    "#Reproducing same results\n",
    "SEED = 2019\n",
    "\n",
    "#Torch\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "TEXT = data.Field(tokenize='spacy',batch_first=True,include_lengths=True)\n",
    "LABEL = data.LabelField(dtype = torch.long,batch_first=True)\n",
    "datafield = {\"text\": (\"text\",TEXT),\n",
    "                     \"label\": (\"label\",LABEL)\n",
    "             }\n",
    "\n",
    "#loading custom dataset\n",
    "training_data = data.TabularDataset(path = 'yelp_review_small.json',format = 'json',fields = datafield)\n",
    "\n",
    "#print preprocessed text\n",
    "print(len(training_data.examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "train_data, test_data = training_data.split(split_ratio=0.7, random_state = random.seed(SEED))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of TEXT vocabulary: 1622\n",
      "Size of LABEL vocabulary: 5\n",
      "[('.', 314), ('the', 208), ('and', 192), (',', 157), ('I', 150), ('to', 136), ('a', 119), ('!', 80), ('was', 80), (' ', 74)]\n",
      "defaultdict(<bound method Vocab._default_unk_index of <torchtext.vocab.Vocab object at 0x1300cc410>>, {'<unk>': 0, '<pad>': 1, '.': 2, 'the': 3, 'and': 4, ',': 5, 'I': 6, 'to': 7, 'a': 8, '!': 9, 'was': 10, ' ': 11, 'it': 12, 'of': 13, 'is': 14, 'for': 15, 'in': 16, 'we': 17, '\\n\\n': 18, 'The': 19, 'but': 20, 'my': 21, 'with': 22, 'that': 23, 'this': 24, 'had': 25, 'on': 26, 'have': 27, \"n't\": 28, 'you': 29, 'place': 30, '-': 31, 'out': 32, 'they': 33, 'be': 34, 'good': 35, \"'s\": 36, 'just': 37, 'at': 38, 'not': 39, 'were': 40, 'We': 41, 'are': 42, '\"': 43, 'all': 44, 'our': 45, 'so': 46, 'back': 47, 'do': 48, 'one': 49, 'service': 50, 'food': 51, 'get': 52, '(': 53, ')': 54, 'as': 55, 'can': 56, 'he': 57, 'would': 58, '?': 59, 'great': 60, 'did': 61, 'from': 62, 'very': 63, '\\n': 64, 'go': 65, 'if': 66, 'up': 67, 'car': 68, 'like': 69, 'nice': 70, 'only': 71, 'there': 72, '...': 73, 'It': 74, 'me': 75, 'or': 76, 'right': 77, 'time': 78, 'will': 79, 'My': 80, 'They': 81, 'an': 82, 'menu': 83, 'no': 84, 'their': 85, 'try': 86, 'because': 87, 'chicken': 88, 'location': 89, 'love': 90, 'what': 91, 'about': 92, 'even': 93, 'here': 94, 'made': 95, 'pizza': 96, 'really': 97, 'when': 98, \"'re\": 99, 'This': 100, 'been': 101, 'dinner': 102, 'going': 103, 'know': 104, 'more': 105, 'never': 106, 'oil': 107, 'restaurant': 108, 'said': 109, 'say': 110, 'she': 111, 'still': 112, 'than': 113, 'us': 114, 'your': 115, '2': 116, 'So': 117, 'after': 118, 'again': 119, 'also': 120, 'am': 121, 'amazing': 122, 'around': 123, 'came': 124, 'change': 125, 'come': 126, 'could': 127, 'day': 128, 'got': 129, 'has': 130, 'hot': 131, 'recommend': 132, 'since': 133, 'way': 134, 'which': 135, 'who': 136, \"'\": 137, 'Indian': 138, 'always': 139, 'before': 140, 'being': 141, 'best': 142, 'better': 143, 'definitely': 144, 'does': 145, 'everything': 146, 'experience': 147, 'fresh': 148, 'give': 149, 'husband': 150, 'make': 151, 'many': 152, 'order': 153, 'ordered': 154, 'seafood': 155, 'sure': 156, 'them': 157, 'wait': 158, 'week': 159, 'well': 160, 'went': 161, '$': 162, \"'m\": 163, '3': 164, 'Great': 165, 'Very': 166, 'When': 167, 'any': 168, 'asked': 169, 'bad': 170, 'bit': 171, 'business': 172, 'by': 173, 'clean': 174, 'coming': 175, 'cut': 176, 'done': 177, 'down': 178, 'ever': 179, 'every': 180, 'few': 181, 'finally': 182, 'first': 183, 'friendly': 184, 'hours': 185, 'job': 186, 'little': 187, 'looking': 188, 'much': 189, 'over': 190, 'pretty': 191, 'quality': 192, 'room': 193, 'should': 194, 'staff': 195, 'table': 196, 'thing': 197, 'told': 198, 'too': 199, 'while': 200, 'work': 201, \"'ll\": 202, ':': 203, ';': 204, 'A': 205, 'All': 206, 'As': 207, 'If': 208, 'Service': 209, 'You': 210, 'acai': 211, 'almost': 212, 'another': 213, 'area': 214, 'beautiful': 215, 'biryani': 216, 'ca': 217, 'called': 218, 'course': 219, 'decided': 220, 'decor': 221, 'different': 222, 'disappointed': 223, 'engine': 224, 'excellent': 225, 'far': 226, 'favorite': 227, 'full': 228, 'fun': 229, 'her': 230, 'him': 231, 'home': 232, 'hotel': 233, 'instead': 234, 'last': 235, 'left': 236, 'less': 237, 'mozzarella': 238, 'new': 239, 'okay': 240, 'options': 241, 'other': 242, 'past': 243, 'people': 244, 'prices': 245, 'rice': 246, 'sauce': 247, 'shrimp': 248, 'small': 249, 'smoothie': 250, 'some': 251, 'son': 252, 'take': 253, 'think': 254, 'took': 255, 'two': 256, 'until': 257, 'waited': 258, 'waiting': 259, 'want': 260, 'wanted': 261, 'weeks': 262, 'working': 263, 'year': 264, 'yummy': 265, \"'d\": 266, \"'ve\": 267, '4': 268, '5': 269, 'After': 270, 'And': 271, 'Asian': 272, 'At': 273, 'Beef': 274, 'Definitely': 275, 'Delivery': 276, 'Even': 277, 'Hill': 278, 'NOT': 279, 'No': 280, 'Our': 281, 'Scottsdale': 282, 'Some': 283, 'Sunday': 284, 'THE': 285, 'That': 286, 'There': 287, 'Will': 288, 'able': 289, 'anyone': 290, 'apartment': 291, 'appointment': 292, 'ask': 293, 'atmosphere': 294, 'away': 295, 'banana': 296, 'black': 297, 'bowl': 298, 'breakfast': 299, 'bring': 300, 'cheap': 301, 'comes': 302, 'company': 303, 'customer': 304, 'daal': 305, 'daughter': 306, 'delicious': 307, 'die': 308, 'door': 309, 'downside': 310, 'drive': 311, 'eat': 312, 'enough': 313, 'expect': 314, 'filter': 315, 'find': 316, 'friend': 317, 'fries': 318, 'front': 319, 'furniture': 320, 'grab': 321, 'greeted': 322, 'hair': 323, 'haircut': 324, 'hand': 325, 'high': 326, 'highly': 327, 'his': 328, 'hit': 329, 'hood': 330, 'house': 331, 'how': 332, 'i': 333, 'into': 334, 'later': 335, 'light': 336, 'lunch': 337, 'mains': 338, 'maybe': 339, 'meal': 340, 'minutes': 341, 'morning': 342, 'most': 343, 'music': 344, 'night': 345, 'noticed': 346, 'off': 347, 'office': 348, 'others': 349, 'parking': 350, 'part': 351, 'phone': 352, 'pre': 353, 'price': 354, 'priced': 355, 'professional': 356, 'quickly': 357, 'real': 358, 'return': 359, 'same': 360, 'server': 361, 'short': 362, 'side': 363, 'someone': 364, 'special': 365, 'spot': 366, 'strawberry': 367, 'tasty': 368, 'tech': 369, 'tell': 370, 'then': 371, 'those': 372, 'thought': 373, 'today': 374, 'town': 375, 'trash': 376, 'tried': 377, 'used': 378, 'usual': 379, 'waitress': 380, 'walked': 381, 'whole': 382, 'wings': 383, 'wo': 384, 'worth': 385, '  ': 386, '%': 387, '1': 388, '10': 389, ':)': 390, 'American': 391, 'Broccoli': 392, 'CAM': 393, 'Chicken': 394, 'Chipotle': 395, 'Cleats': 396, 'Do': 397, 'Everything': 398, 'FORD': 399, 'Food': 400, 'Ford': 401, 'Fried': 402, 'HUNTERSVILLE': 403, 'Had': 404, 'Huntersville': 405, 'In': 406, 'Just': 407, 'Maria': 408, 'New': 409, 'OF': 410, 'Oh': 411, 'Overall': 412, 'Pizza': 413, 'Place': 414, 'Really': 415, 'Richmond': 416, 'Saturday': 417, 'Scaddabush': 418, 'TV': 419, 'Their': 420, 'Unfortunately': 421, 'Would': 422, 'Year': 423, 'actually': 424, 'added': 425, 'already': 426, 'answer': 427, 'arrive': 428, 'authentic': 429, 'average': 430, 'balls': 431, 'beer': 432, 'bhaji': 433, 'big': 434, 'bland': 435, 'both': 436, 'boyfriend': 437, 'brunch': 438, 'buffet': 439, 'bugs': 440, 'burning': 441, 'busy': 442, 'cheese': 443, 'choice': 444, 'chose': 445, 'cleaning': 446, 'clippers': 447, 'close': 448, 'closer': 449, 'complaints': 450, 'completely': 451, 'complex': 452, 'cozy': 453, 'crab': 454, 'crust': 455, 'dealership': 456, 'delivered': 457, 'delivery': 458, 'dessert': 459, 'dining': 460, 'disgusting': 461, 'dish': 462, 'doing': 463, 'due': 464, 'each': 465, 'early': 466, 'eating': 467, 'emergency': 468, 'ended': 469, 'entrees': 470, 'establishment': 471, 'fairly': 472, 'fast': 473, 'felt': 474, 'filling': 475, 'flammable': 476, 'flavor': 477, 'floor': 478, 'forget': 479, 'forward': 480, 'found': 481, 'freaking': 482, 'friends': 483, 'girl': 484, 'glad': 485, 'goes': 486, 'greens': 487, 'grub': 488, 'happy': 489, 'hard': 490, 'helped': 491, 'horrible': 492, 'however': 493, 'idea': 494, 'jeans': 495, 'joint': 496, 'knew': 497, 'koftas': 498, 'large': 499, 'lbs': 500, 'least': 501, 'leave': 502, 'level': 503, 'lights': 504, 'line': 505, 'list': 506, 'lobster': 507, 'long': 508, 'looked': 509, 'looks': 510, 'lost': 511, 'lot': 512, 'lots': 513, 'loved': 514, 'main': 515, 'major': 516, 'makes': 517, 'male': 518, 'manager': 519, 'masala': 520, 'may': 521, 'meals': 522, 'mid': 523, 'mile': 524, 'months': 525, 'must': 526, 'myself': 527, 'nails': 528, 'needed': 529, 'next': 530, 'now': 531, 'old': 532, 'once': 533, 'outside': 534, 'oysters': 535, 'pair': 536, 'parked': 537, 'pav': 538, 'pay': 539, 'perfect': 540, 'person': 541, 'picture': 542, 'placed': 543, 'places': 544, 'plain': 545, 'plenty': 546, 'point': 547, 'pop': 548, 'popped': 549, 'possibly': 550, 'pricey': 551, 'probably': 552, 'purchased': 553, 'put': 554, 'quite': 555, 'rather': 556, 'reasonable': 557, 'reasonably': 558, 'repaired': 559, 'reservations': 560, 'roll': 561, 'round': 562, 'salad': 563, 'section': 564, 'see': 565, 'seemed': 566, 'serve': 567, 'sets': 568, 'shipment': 569, 'simple': 570, 'sit': 571, 'slices': 572, 'slow': 573, 'smoke': 574, 'sofa': 575, 'sometimes': 576, 'somewhere': 577, 'spicy': 578, 'star': 579, 'start': 580, 'started': 581, 'state': 582, 'station': 583, 'step': 584, 'stop': 585, 'store': 586, 'strip': 587, 'sub': 588, 'such': 589, 'suite': 590, 'summer': 591, 'tables': 592, 'tacos': 593, 'taking': 594, 'techs': 595, 'though': 596, 'thru': 597, 'tikka': 598, 'times': 599, 'top': 600, 'trying': 601, 'tub': 602, 'twice': 603, 'under': 604, 'use': 605, 'vegan': 606, 'vegetables': 607, 'wholesome': 608, 'why': 609, 'wonderful': 610, 'worked': 611, 'yes': 612, '\\n \\n': 613, ' \\n\\n': 614, '   ': 615, '#': 616, '+': 617, '..': 618, '....': 619, '.....': 620, '......': 621, '/': 622, '12': 623, '15': 624, '15515': 625, '16': 626, '20': 627, '2000': 628, '21st': 629, '22': 630, '24': 631, '25': 632, '30': 633, '40': 634, '41.00': 635, '5:15': 636, '6': 637, '7': 638, '85¢': 639, '8:15': 640, '99': 641, '9:05': 642, ';)': 643, 'ALL': 644, 'AMAZING': 645, 'ANY': 646, 'AS': 647, 'Actually': 648, 'Adam': 649, 'Also': 650, 'Although': 651, 'Always': 652, 'America': 653, 'Another': 654, 'Anything': 655, 'Anyway': 656, 'April': 657, 'Arizona': 658, 'Attached': 659, 'Authentic': 660, 'Awesome': 661, 'B': 662, 'BACK': 663, 'BALL': 664, 'BBQ': 665, 'Back': 666, 'Bay': 667, 'Believe': 668, 'Bollywood': 669, 'Bolognese': 670, 'Both': 671, 'Bottom': 672, 'Bowlology': 673, 'Brewsters': 674, 'C': 675, 'CAP': 676, 'COMPONENTS': 677, 'Cafe': 678, 'Calgary': 679, 'Called': 680, 'Cane': 681, 'Canes': 682, 'Cashews': 683, 'Centennial': 684, 'Chaching': 685, 'Chilli': 686, 'Chinese': 687, 'Chipolte': 688, 'Choolah': 689, 'Cleveland': 690, 'Clueless': 691, 'Country': 692, 'Crab': 693, 'Crispy': 694, 'Cuco': 695, 'Curries': 696, 'DID': 697, 'DO': 698, 'Deagan': 699, 'Dean': 700, 'Decent': 701, 'Denver': 702, 'Dermatology': 703, 'Diced': 704, 'Dismal': 705, 'Does': 706, 'Domino': 707, 'Drive': 708, 'Easily': 709, 'Either': 710, 'End': 711, 'English': 712, 'Enjoying': 713, 'Eric': 714, 'Eve': 715, 'Every': 716, 'FAIL': 717, 'FILLER': 718, 'FOR': 719, 'Familiy': 720, 'Feast': 721, 'First': 722, 'Fishermen': 723, 'For': 724, 'Friday': 725, 'Fruits': 726, 'GO': 727, 'GREAT': 728, 'Garlic': 729, 'Garling': 730, 'Get': 731, 'Gilbert': 732, 'Ginger': 733, 'Gold': 734, 'HERE': 735, 'HOLD': 736, 'Ha': 737, 'Happy': 738, 'Has': 739, 'Heather': 740, 'Highly': 741, 'Hip': 742, 'Holy': 743, 'Hop': 744, 'Hoped': 745, 'Hot': 746, 'House': 747, 'How': 748, 'However': 749, 'Hut': 750, 'I-85': 751, 'IT': 752, 'Keep': 753, 'Kumi': 754, 'LOVE': 755, 'LOVED': 756, 'Landlord': 757, 'Lao': 758, 'Like': 759, 'Located': 760, 'Love': 761, 'MAKE': 762, 'MY': 763, 'Mac': 764, 'Madison': 765, 'Make': 766, 'Manager': 767, 'Mandalay': 768, 'Margherita': 769, 'Martin': 770, 'Mexican': 771, 'Mile': 772, 'Mizuno': 773, 'More': 774, 'Most': 775, 'Moved': 776, 'Mumbly': 777, 'N': 778, 'NCAA': 779, 'NEVER': 780, 'NO': 781, 'Naked': 782, 'Never': 783, 'Next': 784, 'Nice': 785, 'Noise': 786, 'Not': 787, 'Now': 788, 'OIL': 789, 'ON': 790, 'ORDER': 791, 'OTHER': 792, 'Oil': 793, 'On': 794, 'Once': 795, 'One': 796, 'Onions': 797, 'PA': 798, 'PD': 799, 'PH': 800, 'PUT': 801, 'Paid': 802, 'Paint': 803, 'Paired': 804, 'Palms': 805, 'Pasta': 806, 'Penne': 807, 'Per': 808, 'Phoenix': 809, 'Pittsburgh': 810, 'Please': 811, 'Pool': 812, 'PoolServ': 813, 'Possibly': 814, 'Prices': 815, 'Probably': 816, 'Properties': 817, 'Quarter': 818, 'RESPONSIBLE': 819, 'RESULT': 820, 'RIGHT': 821, 'RUN': 822, 'Radio': 823, 'Rather': 824, 'Reathrey': 825, 'Red': 826, 'Rent': 827, 'Review': 828, 'Ribs': 829, 'Right': 830, 'Road': 831, 'Room': 832, 'Roots': 833, 'SERVICE': 834, 'SUPER': 835, 'Salad': 836, 'Scallops': 837, 'See': 838, 'Sekong': 839, 'She': 840, 'Simply': 841, 'Since': 842, 'Soup': 843, 'South': 844, 'Staff': 845, 'Such': 846, 'TECHNICIAN': 847, 'THAT': 848, 'THIS': 849, 'TOP': 850, 'TexMex': 851, 'Texas': 852, 'Thai': 853, 'Tip': 854, 'Tldr': 855, 'To': 856, 'Today': 857, 'Ton': 858, 'Town': 859, 'Traveler': 860, 'Tried': 861, 'Try': 862, 'Update': 863, 'Using': 864, 'VERY': 865, 'Vancouver': 866, 'WAY': 867, 'WILL': 868, 'WORST': 869, 'WRONG': 870, 'Was': 871, 'Waunakee': 872, 'Weary': 873, 'Went': 874, 'What': 875, 'Whew': 876, 'Who': 877, 'Wild': 878, 'Wing': 879, 'Within': 880, 'Wo': 881, 'Won': 882, 'Wow': 883, 'YOUR': 884, 'Yard': 885, 'Yelp': 886, 'Yes': 887, 'Yuuuuuuuuuuuuummmmmmmmmaaaaaeeeeeee': 888, 'Zumba': 889, 'above': 890, 'absolute': 891, 'absurd': 892, 'accessible': 893, 'accommodating': 894, 'according': 895, 'accumulation': 896, 'acknowledged': 897, 'across': 898, 'activities': 899, 'advised': 900, 'advisor': 901, 'afternoon': 902, 'against': 903, 'ago': 904, 'agreement': 905, 'air': 906, 'airport': 907, 'albeit': 908, 'alcohol': 909, 'alert': 910, 'alleged': 911, 'along': 912, 'altered': 913, 'amateur': 914, 'ambiance': 915, 'amenities': 916, 'amongst': 917, 'angry': 918, 'animal': 919, 'ants': 920, 'anything': 921, 'apart': 922, 'apologizing': 923, 'appetizer': 924, 'arrived': 925, 'arriving': 926, 'artistry': 927, 'ass': 928, 'assistant': 929, 'assumed': 930, 'assure': 931, 'attempt': 932, 'attendants': 933, 'attentive': 934, 'attitude': 935, 'authentically': 936, 'availability': 937, 'awesome': 938, 'bagel': 939, 'balance': 940, 'balances': 941, 'ball': 942, 'barely': 943, 'bass': 944, 'bath': 945, 'bathroom': 946, 'bathrooms': 947, 'beef': 948, 'benny': 949, 'bite': 950, 'bites': 951, 'blows': 952, 'blueberries': 953, 'bothers': 954, 'bowls': 955, 'boys': 956, 'brake': 957, 'branch': 958, 'broth': 959, 'budget': 960, 'building': 961, 'bullet': 962, 'bunch': 963, 'burger': 964, 'burrito': 965, 'butt': 966, 'butter': 967, 'buys': 968, 'cabin': 969, 'call': 970, 'callbacks': 971, 'calls': 972, 'canes': 973, 'care': 974, 'carry': 975, 'casa': 976, 'case': 977, 'cashiers': 978, 'casino': 979, 'century': 980, 'chain': 981, 'challenge': 982, 'changed': 983, 'check': 984, 'checking': 985, 'chef': 986, 'childhood': 987, 'chilli': 988, 'chip': 989, 'chitlins': 990, 'chlorine': 991, 'chocolate': 992, 'choices': 993, 'choose': 994, 'churn': 995, 'chutney': 996, 'citrus': 997, 'city': 998, 'classes': 999, 'classical': 1000, 'classics': 1001, 'cleats': 1002, 'clipper': 1003, 'closed': 1004, 'cockroaches': 1005, 'cocktails': 1006, 'college': 1007, 'commented': 1008, 'communicate': 1009, 'communication': 1010, 'commute': 1011, 'commuter': 1012, 'companies': 1013, 'compared': 1014, 'complaint': 1015, 'completed': 1016, 'concerns': 1017, 'congee': 1018, 'consider': 1019, 'constantly': 1020, 'contact': 1021, 'continued': 1022, 'cook': 1023, 'cooked': 1024, 'cookies': 1025, 'cooling': 1026, 'corned': 1027, 'corner': 1028, 'cosmetology': 1029, 'counter': 1030, 'couple': 1031, 'covered': 1032, 'craved': 1033, 'craving': 1034, 'crawling': 1035, 'crazy': 1036, 'crema': 1037, 'crew': 1038, 'critters': 1039, 'crowded': 1040, 'crunch': 1041, 'cuisine': 1042, 'curry': 1043, 'customize': 1044, 'cute': 1045, 'daily': 1046, 'damn': 1047, 'danger': 1048, 'days': 1049, 'dealer': 1050, 'decently': 1051, 'decides': 1052, 'default': 1053, 'definate': 1054, 'defrosted': 1055, 'degrease': 1056, 'deliver': 1057, 'department': 1058, 'depending': 1059, 'deserts': 1060, 'deserves': 1061, 'designed': 1062, 'designer': 1063, 'desk': 1064, 'differmet': 1065, 'dipping': 1066, 'direct': 1067, 'directly': 1068, 'dirty': 1069, 'disappoint': 1070, 'discarded': 1071, 'discovered': 1072, 'dishes': 1073, 'display': 1074, 'dive': 1075, 'dodge': 1076, 'dogs': 1077, 'doors': 1078, 'downward': 1079, 'drank': 1080, 'dressed': 1081, 'drink': 1082, 'driver': 1083, 'drove': 1084, 'dry': 1085, 'during': 1086, 'eaten': 1087, 'effort': 1088, 'egg': 1089, 'electronics': 1090, 'email': 1091, 'emailed': 1092, 'emails': 1093, 'employees': 1094, 'ending': 1095, 'enjoy': 1096, 'enjoyed': 1097, 'entering': 1098, 'entertainment': 1099, 'established': 1100, 'evans': 1101, 'event': 1102, 'except': 1103, 'exceptionally': 1104, 'exchanged': 1105, 'excited': 1106, 'excuse': 1107, 'executes': 1108, 'exhaust': 1109, 'explain': 1110, 'extinguishers': 1111, 'extra': 1112, 'eyed': 1113, 'face': 1114, 'facility': 1115, 'fact': 1116, 'fair': 1117, 'fajita': 1118, 'family': 1119, 'fantastic': 1120, 'fault': 1121, 'fee': 1122, 'feeling': 1123, 'fella': 1124, 'festive': 1125, 'fill': 1126, 'finding': 1127, 'fine': 1128, 'finishes': 1129, 'fire': 1130, 'fired': 1131, 'fit': 1132, 'fix': 1133, 'fixing': 1134, 'fixings': 1135, 'flavorful': 1136, 'flush': 1137, 'follow': 1138, 'following': 1139, 'free': 1140, 'freshest': 1141, 'fried': 1142, 'frivolously': 1143, 'funky': 1144, 'funniest': 1145, 'fusion': 1146, 'game': 1147, 'garbage': 1148, 'garlic': 1149, 'gas': 1150, 'gave': 1151, 'gear': 1152, 'generous': 1153, 'germs': 1154, 'getting': 1155, 'giant': 1156, 'girls': 1157, 'given': 1158, 'glop': 1159, 'gorgeous': 1160, 'granola': 1161, 'green': 1162, 'gripe': 1163, 'griping': 1164, 'guac': 1165, 'guests': 1166, 'guy': 1167, 'guys': 1168, 'gym': 1169, 'ha': 1170, 'half': 1171, 'hands': 1172, 'happily': 1173, 'head': 1174, 'healthyish': 1175, 'heck': 1176, 'help': 1177, 'helpful': 1178, 'hesistant': 1179, 'holes': 1180, 'hope': 1181, 'hopefully': 1182, 'hoping': 1183, 'horrified': 1184, 'hospitable': 1185, 'hospitality': 1186, 'hostess': 1187, 'hotline': 1188, 'hour': 1189, 'huge': 1190, 'hype': 1191, 'ignite': 1192, 'ignored': 1193, 'immediately': 1194, 'impressed': 1195, 'included': 1196, 'includes': 1197, 'industry': 1198, 'ingredients': 1199, 'inside': 1200, 'interestingly': 1201, 'interiors': 1202, 'invade': 1203, 'issue': 1204, 'issues': 1205, 'items': 1206, 'its': 1207, 'jail': 1208, 'jean': 1209, 'jumped': 1210, 'keep': 1211, 'kept': 1212, 'kid': 1213, 'kids': 1214, 'kinds': 1215, 'kitchens': 1216, 'knowledgeable': 1217, 'known': 1218, 'lackluster': 1219, 'late': 1220, 'laws': 1221, 'lease': 1222, 'lemon': 1223, 'license': 1224, 'limited': 1225, 'liter': 1226, 'live': 1227, 'lived': 1228, 'lizards': 1229, 'located': 1230, 'locations': 1231, 'lock': 1232, 'lol': 1233, 'look': 1234, 'loud': 1235, 'lousy': 1236, 'lucked': 1237, 'luckily': 1238, 'luggage': 1239, 'lukewarm': 1240, 'lure': 1241, 'man': 1242, 'mango': 1243, 'manifold': 1244, 'manners': 1245, 'mean': 1246, 'means': 1247, 'meatball': 1248, 'meatballs': 1249, 'mechanics': 1250, 'medium': 1251, 'meet': 1252, 'metal': 1253, 'mexican': 1254, 'middle': 1255, 'might': 1256, 'mill': 1257, 'min': 1258, 'mind': 1259, 'mins': 1260, 'miss': 1261, 'mix': 1262, 'mixed': 1263, 'motor': 1264, 'move': 1265, 'moves': 1266, 'moving': 1267, 'n': 1268, 'nail': 1269, 'name': 1270, 'napkins': 1271, 'narrow': 1272, 'near': 1273, 'needs': 1274, 'newly': 1275, 'nibbled': 1276, 'nights': 1277, 'nite': 1278, 'none': 1279, 'nope': 1280, 'nor': 1281, 'normally': 1282, 'note': 1283, 'nothing': 1284, 'occasion': 1285, 'occasions': 1286, 'offered': 1287, 'offerings': 1288, 'older': 1289, 'omelette': 1290, 'onions': 1291, 'onto': 1292, 'open': 1293, 'opened': 1294, 'opposite': 1295, 'opted': 1296, 'option': 1297, 'orgasm': 1298, 'otherwise': 1299, 'ours': 1300, 'outing': 1301, 'outstanding': 1302, 'overhaul': 1303, 'overhear': 1304, 'own': 1305, 'owner': 1306, 'packed': 1307, 'paid': 1308, 'pairs': 1309, 'palate': 1310, 'paper': 1311, 'par': 1312, 'partake': 1313, 'partner': 1314, 'party': 1315, 'pasta': 1316, 'patio': 1317, 'patronage': 1318, 'peas': 1319, 'peppers': 1320, 'perfection': 1321, 'perform': 1322, 'perfunctorily': 1323, 'period': 1324, 'personality': 1325, 'pickup': 1326, 'pictures': 1327, 'piece': 1328, 'pieces': 1329, 'piping': 1330, 'pita': 1331, 'pitaya': 1332, 'pitches': 1333, 'planning': 1334, 'plated': 1335, 'plates': 1336, 'platter': 1337, 'pleasant': 1338, 'pleasantly': 1339, 'pleased': 1340, 'pleasure': 1341, 'pleather': 1342, 'pocket': 1343, 'pool': 1344, 'poor': 1345, 'portion': 1346, 'positive': 1347, 'potentially': 1348, 'pouring': 1349, 'prefer': 1350, 'prepared': 1351, 'presentation': 1352, 'presented': 1353, 'pricing': 1354, 'prior': 1355, 'problem': 1356, 'procedures': 1357, 'promised': 1358, 'protection': 1359, 'provided': 1360, 'pulled': 1361, 'purchasing': 1362, 'queen': 1363, 'queso': 1364, 'question': 1365, 'quick': 1366, 'quietly': 1367, 'rate': 1368, 'reach': 1369, 'reached': 1370, 'read': 1371, 'realize': 1372, 'realized': 1373, 'receive': 1374, 'received': 1375, 'recently': 1376, 'recommendation': 1377, 'recommended': 1378, 'redone': 1379, 'refuse': 1380, 'regretfully': 1381, 'remember': 1382, 'remembered': 1383, 'reminded': 1384, 'removed': 1385, 'rent': 1386, 'replaced': 1387, 'request': 1388, 'requested': 1389, 'requests': 1390, 'required': 1391, 'reservation': 1392, 'respond': 1393, 'responded': 1394, 'responsibility': 1395, 'review': 1396, 'reviewing': 1397, 'reviews': 1398, 'ribs': 1399, 'rich': 1400, 'rolls': 1401, 'rooms': 1402, 'rotation': 1403, 'rounds': 1404, 'row': 1405, 'rubs': 1406, 'run': 1407, 's': 1408, \"s'mores\": 1409, 'salmon': 1410, 'salon': 1411, 'salsa': 1412, 'salsas': 1413, 'sample': 1414, 'sandwich': 1415, 'sandwiches': 1416, 'sauces': 1417, 'saucy': 1418, 'saw': 1419, 'saying': 1420, 'scallops': 1421, 'screaming': 1422, 'screen': 1423, 'screens': 1424, 'scummy': 1425, 'seams': 1426, 'seat': 1427, 'seated': 1428, 'second': 1429, 'seeing': 1430, 'seen': 1431, 'selection': 1432, 'sensor': 1433, 'sent': 1434, 'seriously': 1435, 'serving': 1436, 'set': 1437, 'settled': 1438, 'several': 1439, 'share': 1440, 'shared': 1441, 'shirts': 1442, 'shop': 1443, 'shot': 1444, 'shoved': 1445, 'shower': 1446, 'showing': 1447, 'showroom': 1448, 'sides': 1449, 'signs': 1450, 'simply': 1451, 'single': 1452, 'singled': 1453, 'sitting': 1454, 'sized': 1455, 'sizzling': 1456, 'skeptical': 1457, 'slathered': 1458, 'slice': 1459, 'sliding': 1460, 'smell': 1461, 'smile': 1462, 'smoothies': 1463, 'soft': 1464, 'softball': 1465, 'solid': 1466, 'something': 1467, 'sooooooooo': 1468, 'sophisticated': 1469, 'soup': 1470, 'southwestern': 1471, 'spa': 1472, 'speak': 1473, 'speaks': 1474, 'specials': 1475, 'specific': 1476, 'spice': 1477, 'spices': 1478, 'spiders': 1479, 'spilled': 1480, 'splurge': 1481, 'spray': 1482, 'sprinkle': 1483, 'stale': 1484, 'standard': 1485, 'standbys': 1486, 'stars': 1487, 'stay': 1488, 'stayed': 1489, 'staying': 1490, 'steam': 1491, 'sticking': 1492, 'sticks': 1493, 'stomachs': 1494, 'stones': 1495, 'stopped': 1496, 'stopping': 1497, 'story': 1498, 'street': 1499, 'strong': 1500, 'stuff': 1501, 'stumble': 1502, 'style': 1503, 'stylist': 1504, 'stylists': 1505, 'subway': 1506, 'super': 1507, 'surprised': 1508, 'surprisingly': 1509, 'sushi': 1510, 'swear': 1511, 'sweet': 1512, 'switch': 1513, 'symmetrical': 1514, 'ta': 1515, 'tacky': 1516, 'tailgate': 1517, 'tails': 1518, 'taken': 1519, 'takes': 1520, 'talked': 1521, 'taped': 1522, 'taste': 1523, 'tasted': 1524, 'tastes': 1525, 'tasting': 1526, 'technician': 1527, 'tender': 1528, 'texture': 1529, 'than10': 1530, 'thank': 1531, 'thanked': 1532, 'these': 1533, 'thin': 1534, 'things': 1535, 'thinking': 1536, 'three': 1537, 'threw': 1538, 'through': 1539, 'throw': 1540, 'tighten': 1541, 'tiny': 1542, 'tip': 1543, 'tired': 1544, 'tongs': 1545, 'tons': 1546, 'topping': 1547, 'tossed': 1548, 'total': 1549, 'totally': 1550, 'touch': 1551, 'traditional': 1552, 'trained': 1553, 'training': 1554, 'transmission': 1555, 'travel': 1556, 'tray': 1557, 'treat': 1558, 'trifecta': 1559, 'trip': 1560, 'troubles': 1561, 'true': 1562, 'truly': 1563, 'turn': 1564, 'turned': 1565, 'type': 1566, 'typically': 1567, 'unengaged': 1568, 'units': 1569, 'university': 1570, 'unplugged': 1571, 'unsatisfied': 1572, 'unthawed': 1573, 'upon': 1574, 'upscale': 1575, 'upside': 1576, 'usually': 1577, 'utilities': 1578, 'variety': 1579, 'veggie': 1580, 'view': 1581, 'visits': 1582, 'voicemail': 1583, 'w/': 1584, 'waist': 1585, 'waiter': 1586, 'walk': 1587, 'wall': 1588, 'wants': 1589, 'warning': 1590, 'waste': 1591, 'watch': 1592, 'watching': 1593, 'weee': 1594, 'weekday': 1595, 'weekend': 1596, 'weight': 1597, 'welcoming': 1598, 'wells': 1599, 'west': 1600, 'whatever': 1601, 'wheel': 1602, 'where': 1603, 'wierd': 1604, 'wing': 1605, 'wise': 1606, 'wish': 1607, 'won': 1608, 'wood': 1609, 'workers': 1610, 'worry': 1611, 'worst': 1612, 'worthwhile': 1613, 'writing': 1614, 'y': 1615, 'yeah': 1616, 'yellow': 1617, 'yep': 1618, 'yesterday': 1619, 'yet': 1620, 'zips': 1621})\n"
     ]
    }
   ],
   "source": [
    "#initialize glove embeddings\n",
    "TEXT.build_vocab(train_data,min_freq=1,vectors = \"glove.6B.100d\")\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "#No. of unique tokens in text\n",
    "print(\"Size of TEXT vocabulary:\",len(TEXT.vocab))\n",
    "\n",
    "#No. of unique tokens in label\n",
    "print(\"Size of LABEL vocabulary:\",len(LABEL.vocab))\n",
    "\n",
    "#Commonly used words\n",
    "print(TEXT.vocab.freqs.most_common(10))\n",
    "\n",
    "#Word dictionary\n",
    "print(TEXT.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ilannaiman/opt/anaconda3/envs/torchDP/lib/python3.7/site-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "#set batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "#Load an iterator\n",
    "train_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, test_data),\n",
    "    batch_size = BATCH_SIZE,\n",
    "    sort_key = lambda x: len(x.text),\n",
    "    sort_within_batch=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN_setiment(nn.Module):\n",
    "    \n",
    "    ## For each element in the input sequence, each layer computes the following function:\n",
    "    \n",
    "    ## h_t = ReLU(W_ih*x_t+b_ih + W_hh*h_(t-1)+b_hh)\n",
    "    \n",
    "    ## where h_t is the hidden state at time t, x_t is the input at time t, and h_(t-1)\n",
    "    ## is the hidden state of the previous layer at time t-1 or the initial hidden state at time 0\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers):\n",
    "\n",
    "        #Constructor\n",
    "        super().__init__()          \n",
    "\n",
    "        #embedding layer\n",
    "        # A simple lookup table that stores embeddings of a fixed dictionary and size.\n",
    "        # This module is often used to store word embeddings and retrieve them using indices.\n",
    "        # The input to the module is a list of indices, and the output is the corresponding word embeddings.\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        #rnn layer\n",
    "        self.rnn = nn.RNN(embedding_dim, \n",
    "                       hidden_dim, \n",
    "                       num_layers=n_layers,\n",
    "                       batch_first=True)\n",
    "\n",
    "        # linear layer towards output\n",
    "        self.linear = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "        # activation function\n",
    "        self.softmax = nn.LogSoftmax(dim=2)\n",
    "\n",
    "    def forward(self, text, text_length):\n",
    "\n",
    "        embedded = self.embedding(text)\n",
    "        print('text')\n",
    "        print(text)\n",
    "        print('text_length')\n",
    "        print(text_length)\n",
    "        ## input can be of size T x B x * where T is the length of the longest sequence (equal to lengths[0]),\n",
    "        # B is the batch size, and * is any number of dimensions (including 0).\n",
    "        # If batch_first is True, B x T x * input is expected.\n",
    "        packed_embedding = nn.utils.rnn.pack_padded_sequence(embedded, text_length, batch_first=True)\n",
    "        print('packed_embedding:')\n",
    "        print(packed_embedding)\n",
    "        packed_output, hidden = self.rnn(packed_embedding)\n",
    "        print('hidden:')\n",
    "        print(hidden)\n",
    "        linear_outputs = self.linear(hidden)\n",
    "        # print('linear_outputs')\n",
    "        # print(linear_outputs.shape)\n",
    "        # print(linear_outputs)\n",
    "        output = self.softmax(linear_outputs)\n",
    "        # print('output tensor:')\n",
    "        # print(output.shape)\n",
    "        # print(output.view)\n",
    "        # print(output)\n",
    "        return output, hidden"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "\n",
    "# hyper-parameters\n",
    "size_of_vocab = len(TEXT.vocab)\n",
    "embedding_dim = 100\n",
    "num_hidden_nodes = 32\n",
    "num_output_nodes = 5\n",
    "num_layers = 1\n",
    "\n",
    "\n",
    "# instantiate the Rnn sentiment classification model for Yelp\n",
    "model = RNN_setiment(size_of_vocab, embedding_dim, num_hidden_nodes,num_output_nodes, num_layers)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN_setiment(\n",
      "  (embedding): Embedding(1622, 100)\n",
      "  (rnn): RNN(100, 32, batch_first=True)\n",
      "  (linear): Linear(in_features=32, out_features=5, bias=True)\n",
      "  (softmax): LogSoftmax(dim=2)\n",
      ")\n",
      "The model has 166,653 trainable parameters\n",
      "torch.Size([1622, 100])\n"
     ]
    }
   ],
   "source": [
    "#architecture\n",
    "print(model)\n",
    "\n",
    "#No. of trainable parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "\n",
    "#Initialize the pretrained embedding\n",
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "\n",
    "print(pretrained_embeddings.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "import torch.optim as opt\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = opt.Adam(model.parameters())\n",
    "\n",
    "\n",
    "def rate_from_prediction(prediction):\n",
    "    top_n, top_i = prediction.topk(1)\n",
    "    rate_i = top_i[0].item()\n",
    "    return rate_i\n",
    "\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "\n",
    "    # initialize every epoch\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    # set the model in training phase\n",
    "    model.train()\n",
    "\n",
    "    for batch in iterator:\n",
    "\n",
    "        # resets the gradients after every batch\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # retrieve text and no. of words\n",
    "        text, text_lengths = batch.text\n",
    "\n",
    "        # convert to 1D tensor\n",
    "        predictions, _ = model(text, text_lengths).squeeze()\n",
    "        # print('prediction tensor:')\n",
    "        # print(predictions)\n",
    "        # print(predictions.shape)\n",
    "        # print(predictions.view)\n",
    "        # print('batch.label tensor:')\n",
    "        # print(batch.label)\n",
    "        # print(batch.label.shape)\n",
    "        # print(batch.label.view)\n",
    "\n",
    "        # compute the loss\n",
    "        loss = criterion(predictions, batch.label)\n",
    "\n",
    "        # compute the accuracy\n",
    "        guess = rate_from_prediction(predictions)\n",
    "        correct = (guess == batch.label).float()\n",
    "        acc = correct.sum() / len(correct)\n",
    "\n",
    "        #backpropage the loss and compute the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        #update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        #loss and accuracy\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [],
   "source": [
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "\n",
    "    # initialize every epoch\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    # deactivating dropout layers\n",
    "    model.eval()\n",
    "\n",
    "    # deactivates autograd\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for batch in iterator:\n",
    "\n",
    "            #retrieve text and no. of words\n",
    "            text, text_lengths = batch.text\n",
    "\n",
    "            #convert to 1d tensor\n",
    "            predictions = model(text, text_lengths).squeeze()\n",
    "\n",
    "            #compute loss and accuracy\n",
    "            loss = criterion(predictions, batch.label)\n",
    "\n",
    "            guess = rate_from_prediction(predictions)\n",
    "            correct = (guess == batch.label).float()\n",
    "            acc = correct.sum() / len(correct)\n",
    "\n",
    "            #keep track of loss and accuracy\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text\n",
      "tensor([[   6,  217,   28,  ...,  403,  399,    2],\n",
      "        [ 205,  448,  317,  ...,    1,    1,    1],\n",
      "        [  41,  311,  173,  ...,    1,    1,    1],\n",
      "        ...,\n",
      "        [ 167,    6,  163,  ...,    1,    1,    1],\n",
      "        [   6, 1556,  513,  ...,    1,    1,    1],\n",
      "        [ 728,  834,    9,  ...,    1,    1,    1]])\n",
      "text_length\n",
      "tensor([448, 407, 352, 322, 287, 268, 225, 202, 200, 192, 187, 187, 182, 181,\n",
      "        162, 155, 132, 131, 125, 123, 119, 104, 101, 101,  92,  90,  84,  73,\n",
      "         72,  71,  65,  65,  63,  57,  56,  55,  52,  51,  50,  49,  45,  43,\n",
      "         40,  34,  33,  28,  28,  27,  27,  23])\n",
      "packed_embedding:\n",
      "PackedSequence(data=tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.3398,  0.2094,  0.4635,  ..., -0.2339,  0.4730, -0.0288]],\n",
      "       grad_fn=<PackPaddedSequenceBackward>), batch_sizes=tensor([50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
      "        50, 50, 50, 50, 50, 49, 49, 49, 49, 47, 45, 45, 45, 45, 45, 44, 43, 43,\n",
      "        43, 43, 43, 43, 42, 42, 42, 41, 41, 40, 40, 40, 40, 39, 38, 37, 36, 36,\n",
      "        36, 35, 34, 33, 33, 33, 33, 33, 33, 32, 32, 30, 30, 30, 30, 30, 30, 29,\n",
      "        28, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 26, 26, 26, 26, 26, 26,\n",
      "        25, 25, 24, 24, 24, 24, 24, 24, 24, 24, 24, 22, 22, 22, 21, 21, 21, 21,\n",
      "        21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 20, 20, 20, 20, 19, 19, 18,\n",
      "        18, 18, 18, 18, 18, 17, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
      "        16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 15, 15, 15, 15, 15, 15, 15,\n",
      "        14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14,\n",
      "        14, 13, 12, 12, 12, 12, 12, 10, 10, 10, 10, 10,  9,  9,  9,  9,  9,  9,\n",
      "         9,  9,  8,  8,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "         7,  7,  7,  7,  7,  7,  7,  7,  7,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "         6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "         6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  5,  5,\n",
      "         5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  4,\n",
      "         4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "         4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  3,  3,\n",
      "         3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
      "         3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  2,  2,  2,  2,  2,  2,  2,  2,\n",
      "         2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
      "         2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
      "         2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1]), sorted_indices=None, unsorted_indices=None)\n",
      "hidden:\n",
      "tensor([[[-0.5400, -0.4363,  0.1372,  ...,  0.5753, -0.2358,  0.5918],\n",
      "         [-0.4642, -0.5010, -0.5906,  ...,  0.7281, -0.2396,  0.2083],\n",
      "         [-0.4901, -0.5973, -0.7045,  ..., -0.0554, -0.4124, -0.6736],\n",
      "         ...,\n",
      "         [-0.5663, -0.7327, -0.6962,  ..., -0.2606, -0.4984, -0.6721],\n",
      "         [ 0.3893,  0.2459, -0.5172,  ..., -0.0597, -0.7205, -0.0596],\n",
      "         [-0.4291, -0.6877, -0.7315,  ..., -0.3974, -0.3963, -0.7345]]],\n",
      "       grad_fn=<StackBackward>)\n",
      "text\n",
      "tensor([[100, 424, 378,  ..., 146,   0,   2],\n",
      "        [207,   3,   0,  ...,   1,   1,   1],\n",
      "        [406,   3,   0,  ...,   1,   1,   1],\n",
      "        ...,\n",
      "        [761,   3, 195,  ...,   1,   1,   1],\n",
      "        [205,   0,  16,  ...,   1,   1,   1],\n",
      "        [389,   0,  26,  ...,   1,   1,   1]])\n",
      "text_length\n",
      "tensor([818, 558, 335, 306, 252, 190, 171, 161, 156, 154, 126, 117, 110,  63,\n",
      "         63,  58,  48,  38,  29,  27,  25])\n",
      "packed_embedding:\n",
      "PackedSequence(data=tensor([[-6.8881e-04, -2.1182e-04,  4.8749e-04,  ..., -5.0788e-04,\n",
      "          4.4318e-04, -7.3415e-04],\n",
      "        [-1.0403e-28, -1.7002e-29,  5.5078e-30,  ..., -2.8877e-29,\n",
      "          4.9818e-29, -2.8570e-29],\n",
      "        [ 1.6395e-35,  8.4078e-37, -8.4078e-37,  ..., -1.1210e-36,\n",
      "         -1.1210e-35, -3.6434e-36],\n",
      "        ...,\n",
      "        [ 1.2028e-02,  3.3435e-01,  6.2713e-01,  ...,  1.0672e-01,\n",
      "          4.1612e-01,  8.0197e-01],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [-3.4079e-01,  2.1041e-01,  4.6248e-01,  ..., -2.3494e-01,\n",
      "          4.7198e-01, -2.7803e-02]]), batch_sizes=tensor([21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n",
      "        21, 21, 21, 21, 21, 21, 21, 20, 20, 19, 19, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 16, 16, 16, 16, 16, 16,\n",
      "        16, 16, 16, 16, 15, 15, 15, 15, 15, 13, 13, 13, 13, 13, 13, 13, 13, 13,\n",
      "        13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,\n",
      "        13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,\n",
      "        13, 13, 12, 12, 12, 12, 12, 12, 12, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
      "        10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
      "        10, 10, 10, 10, 10, 10, 10, 10, 10, 10,  9,  9,  8,  8,  8,  8,  8,  7,\n",
      "         7,  7,  7,  7,  7,  7,  7,  7,  7,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "         6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  5,  5,  5,  5,  5,  5,  5,  5,\n",
      "         5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,\n",
      "         5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,\n",
      "         5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,\n",
      "         4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "         4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "         4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "         3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
      "         3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  2,  2,  2,  2,  2,  2,  2,\n",
      "         2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
      "         2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
      "         2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
      "         2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
      "         2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
      "         2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
      "         2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
      "         2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
      "         2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
      "         2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
      "         2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
      "         2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1]), sorted_indices=None, unsorted_indices=None)\n",
      "hidden:\n",
      "tensor([[[-0.3562, -0.5610, -0.1800, -0.8564,  0.2211, -0.3585,  0.5217,\n",
      "           0.3587,  0.0325,  0.3278, -0.3912,  0.7147, -0.4357, -0.4109,\n",
      "           0.4843,  0.1583, -0.3531, -0.8801,  0.5601, -0.5823,  0.6063,\n",
      "          -0.5598, -0.6000,  0.3282, -0.1500,  0.5678,  0.3061, -0.8619,\n",
      "           0.8520,  0.5832, -0.0891,  0.4497],\n",
      "         [-0.3063, -0.8415, -0.7677, -0.9205,  0.3000, -0.8689,  0.7104,\n",
      "           0.6016,  0.6764,  0.0881, -0.1873,  0.3486, -0.6108,  0.0089,\n",
      "           0.2424,  0.1160,  0.3474, -0.9495,  0.1063,  0.4698, -0.5033,\n",
      "          -0.5103, -0.0709, -0.5186, -0.3558,  0.3261, -0.7681, -0.9705,\n",
      "           0.5534,  0.0904, -0.8235, -0.3188],\n",
      "         [ 0.0174, -0.4755, -0.0100, -0.8024, -0.1506, -0.4585,  0.5314,\n",
      "           0.0809,  0.3779,  0.2190, -0.1527,  0.6866, -0.6096, -0.1855,\n",
      "           0.4398,  0.2520, -0.0534, -0.8643,  0.4102, -0.6610,  0.5910,\n",
      "          -0.6633, -0.6425, -0.0695, -0.3219,  0.7114,  0.6542, -0.7978,\n",
      "           0.8667,  0.4478, -0.0718,  0.5981],\n",
      "         [-0.4991, -0.5138,  0.0682, -0.7573,  0.0554, -0.2433,  0.4877,\n",
      "           0.4997,  0.1427,  0.3434, -0.3823,  0.6790, -0.3715, -0.5579,\n",
      "           0.5194,  0.3112, -0.1324, -0.8629,  0.6100, -0.6254,  0.5690,\n",
      "          -0.5504, -0.6097,  0.3857, -0.1268,  0.5344,  0.3933, -0.8656,\n",
      "           0.8173,  0.5796, -0.0789,  0.5952],\n",
      "         [-0.6861, -0.4514, -0.5451, -0.4436,  0.0205, -0.2770,  0.6156,\n",
      "           0.8590,  0.5938,  0.1991,  0.2515,  0.7167,  0.0166, -0.3052,\n",
      "           0.1576,  0.4724, -0.6352, -0.4001,  0.4803,  0.1126, -0.3641,\n",
      "          -0.1048,  0.0270,  0.2790,  0.0463,  0.5819, -0.1399, -0.5574,\n",
      "           0.4465, -0.3818, -0.5081, -0.2516],\n",
      "         [-0.1078, -0.3335, -0.5768, -0.7968,  0.3078, -0.5665,  0.6111,\n",
      "           0.1343,  0.5735,  0.2927, -0.1415,  0.7889, -0.6076, -0.4386,\n",
      "           0.6140, -0.3859,  0.1350, -0.6958,  0.3081, -0.4460,  0.2770,\n",
      "          -0.5983, -0.6880,  0.2963, -0.0265,  0.6540,  0.5142, -0.8583,\n",
      "           0.7901,  0.3394, -0.2182,  0.4411],\n",
      "         [-0.6735, -0.3118, -0.7861, -0.3565,  0.0828, -0.3789,  0.6991,\n",
      "           0.7742,  0.7909, -0.2160,  0.5601,  0.6587, -0.2437, -0.4275,\n",
      "          -0.0247, -0.0018, -0.6036, -0.2626,  0.2462,  0.1425, -0.3490,\n",
      "          -0.0743,  0.2731,  0.0463,  0.0136,  0.6733,  0.0667, -0.5013,\n",
      "           0.4219, -0.5511, -0.7164, -0.2803],\n",
      "         [-0.2229, -0.4230, -0.2955, -0.6733,  0.1285, -0.4842,  0.4109,\n",
      "           0.0438,  0.6920,  0.2626, -0.3070,  0.7391, -0.3685, -0.4471,\n",
      "           0.7354, -0.1124,  0.3989, -0.6192,  0.5196, -0.5130,  0.2430,\n",
      "          -0.5885, -0.7530,  0.3882,  0.3923,  0.6002,  0.4476, -0.7618,\n",
      "           0.7530,  0.2608, -0.3200,  0.6683],\n",
      "         [-0.1622, -0.5607, -0.1002, -0.7872,  0.0919, -0.5029,  0.5335,\n",
      "           0.0631,  0.4791,  0.0501, -0.1593,  0.5993, -0.3223, -0.2505,\n",
      "           0.5425,  0.0751,  0.0325, -0.8480,  0.4240, -0.5752,  0.3148,\n",
      "          -0.5396, -0.6862,  0.3604,  0.1302,  0.5340,  0.5085, -0.7965,\n",
      "           0.8119,  0.3886, -0.2605,  0.5301],\n",
      "         [-0.1457, -0.5627, -0.3955, -0.7583,  0.2598, -0.5080,  0.5712,\n",
      "           0.1365,  0.6219,  0.1893, -0.2406,  0.7217, -0.3038, -0.2505,\n",
      "           0.5254,  0.1691,  0.1171, -0.7350,  0.2896, -0.5338,  0.2087,\n",
      "          -0.3316, -0.7376,  0.2529,  0.1781,  0.6454,  0.6250, -0.8660,\n",
      "           0.7257,  0.3022, -0.3187,  0.5167],\n",
      "         [-0.4013, -0.6256, -0.3031, -0.8780,  0.2459, -0.3835,  0.2495,\n",
      "           0.1758,  0.2504,  0.1689, -0.2981,  0.7104, -0.5100, -0.4297,\n",
      "           0.6157,  0.1127,  0.0919, -0.8497,  0.4316, -0.6180,  0.4917,\n",
      "          -0.4864, -0.5867,  0.1321,  0.1303,  0.5386,  0.5741, -0.8562,\n",
      "           0.8603,  0.6706, -0.0140,  0.3857],\n",
      "         [-0.3036, -0.4364,  0.3872, -0.6268, -0.1955, -0.4696,  0.5916,\n",
      "           0.0642,  0.4224,  0.3167, -0.1686,  0.5802, -0.1909, -0.2820,\n",
      "           0.6774,  0.4053,  0.3845, -0.7276,  0.6078, -0.6514,  0.3021,\n",
      "          -0.5855, -0.7057,  0.5342,  0.2591,  0.4771,  0.3538, -0.7521,\n",
      "           0.7930,  0.2473, -0.3242,  0.5377],\n",
      "         [-0.2804, -0.6217, -0.1456, -0.7367,  0.3373, -0.5168,  0.2881,\n",
      "           0.1900,  0.4849,  0.0430, -0.3367,  0.7589, -0.6201, -0.4440,\n",
      "           0.7903,  0.0480, -0.0140, -0.7003,  0.4561, -0.5237,  0.2582,\n",
      "          -0.5252, -0.6624,  0.2370,  0.0821,  0.6585,  0.5535, -0.8272,\n",
      "           0.6878,  0.6036, -0.0973,  0.4722],\n",
      "         [-0.4771, -0.7608, -0.2614, -0.7839,  0.1570, -0.3135,  0.3100,\n",
      "           0.5880,  0.0036,  0.1445, -0.4154,  0.6019, -0.3914, -0.4679,\n",
      "           0.5126,  0.2909, -0.4714, -0.9416,  0.5411, -0.2431,  0.6192,\n",
      "          -0.5653, -0.5436,  0.3009, -0.2797,  0.5420,  0.1570, -0.9467,\n",
      "           0.6780,  0.7050, -0.1924,  0.3939],\n",
      "         [-0.4375, -0.6603, -0.2024, -0.8843,  0.2300, -0.3162,  0.4600,\n",
      "           0.3380, -0.1256,  0.3789, -0.4297,  0.7406, -0.4750, -0.3889,\n",
      "           0.6092,  0.1215, -0.3351, -0.9005,  0.5191, -0.5593,  0.5757,\n",
      "          -0.5523, -0.6102,  0.2893, -0.3505,  0.5808,  0.2434, -0.9153,\n",
      "           0.8192,  0.7054,  0.0813,  0.3502],\n",
      "         [-0.4742, -0.3635,  0.2293, -0.5867,  0.0158, -0.1792,  0.4766,\n",
      "           0.2897,  0.2053,  0.0615, -0.1643,  0.5191, -0.3070, -0.5150,\n",
      "           0.5421,  0.5243,  0.0613, -0.8809,  0.7060, -0.5540,  0.5538,\n",
      "          -0.3823, -0.6679,  0.4851, -0.0610,  0.3426,  0.5371, -0.7508,\n",
      "           0.8967,  0.5178, -0.4841,  0.7306],\n",
      "         [-0.5168, -0.7285, -0.1735, -0.8531,  0.0725, -0.2317,  0.4118,\n",
      "           0.4196, -0.1723,  0.2405, -0.3506,  0.6242, -0.5647, -0.4762,\n",
      "           0.5734,  0.2184, -0.3784, -0.9340,  0.5455, -0.5312,  0.6316,\n",
      "          -0.5409, -0.4622,  0.2699, -0.2707,  0.5052,  0.2066, -0.9168,\n",
      "           0.8092,  0.6740, -0.0770,  0.3588],\n",
      "         [-0.4295, -0.6509, -0.7165, -0.4619, -0.0720, -0.6677,  0.5138,\n",
      "           0.7637,  0.6250, -0.0346,  0.3013,  0.7378, -0.3193, -0.2161,\n",
      "           0.1603,  0.1465, -0.4873, -0.1844,  0.3782,  0.1404, -0.6867,\n",
      "          -0.1045,  0.4305, -0.2618,  0.4908,  0.4742, -0.1268, -0.4458,\n",
      "           0.2879, -0.6214, -0.2875, -0.6177],\n",
      "         [-0.4096, -0.4007, -0.8774, -0.7068,  0.1500, -0.6685,  0.7117,\n",
      "           0.6619,  0.7563, -0.1109,  0.4043,  0.7379, -0.1529,  0.2231,\n",
      "           0.0208,  0.0075, -0.2203, -0.0877,  0.2063,  0.3362, -0.5948,\n",
      "          -0.2485, -0.0425,  0.1728,  0.3183,  0.5597,  0.0199, -0.5539,\n",
      "           0.4529, -0.6490, -0.5140, -0.5148],\n",
      "         [-0.2457,  0.2332, -0.1872, -0.3907,  0.1709, -0.3483,  0.5727,\n",
      "          -0.0019,  0.1258,  0.1079,  0.3549, -0.1063,  0.3099,  0.2954,\n",
      "          -0.2331, -0.5337,  0.3860, -0.2509, -0.1378,  0.0474, -0.6787,\n",
      "          -0.0550, -0.4797,  0.2022,  0.3017,  0.2839, -0.0126, -0.3563,\n",
      "           0.1540, -0.2539, -0.3969,  0.0618],\n",
      "         [-0.6050, -0.5661, -0.5488, -0.4373, -0.0030, -0.8618,  0.5589,\n",
      "           0.0804,  0.8968,  0.3248, -0.3485,  0.5918,  0.4421,  0.1884,\n",
      "           0.3291, -0.1350,  0.3353, -0.3326, -0.0283, -0.2032, -0.2081,\n",
      "          -0.2433,  0.0233, -0.1413, -0.1516,  0.7399,  0.4450, -0.6357,\n",
      "           0.7246, -0.3383, -0.4795,  0.0959]]])\n",
      "\tTrain Loss: 1.760 | Train Acc: 22.00%\n",
      "\t Val. Loss: 1.654 |  Val. Acc: 23.81%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ilannaiman/opt/anaconda3/envs/torchDP/lib/python3.7/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "N_EPOCHS = 1\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    #train the model\n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "\n",
    "    #evaluate the model\n",
    "    valid_loss, valid_acc = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "    #save the best model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
    "\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    find approximate fixed points that are state vectors {h_1*, h_2*, h_3*, ...} where h∗i ≈F(h∗i,x=0)\n",
    "    defining a loss function q = 1/N * ||(h - F(h,0) || _2 ^2\n",
    "    and then minimizing q with respect to hidden states, h, using auto-differentiation methods.\n",
    "    Run this optimization multiple times starting from different initial values of h.\n",
    "    These initial conditions sampled randomly from the distribution of state activations explored by\n",
    "    the trained network, which was done to intentionally sample states related to the operation of the RNN.\n",
    "\"\"\"\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}